{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://outlace.com/Reinforcement-Learning-Part-3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randPair(s,e):\n",
    "    return np.random.randint(s,e), np.random.randint(s,e)\n",
    "\n",
    "#finds an array in the \"depth\" dimension of the grid\n",
    "def findLoc(state, obj):\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            if (state[i,j] == obj).all():\n",
    "                return i,j\n",
    "\n",
    "#Initialize stationary grid, all items are placed deterministically\n",
    "def initGrid():\n",
    "    state = np.zeros((4,4,4))\n",
    "    #place player\n",
    "    state[0,1] = np.array([0,0,0,1])\n",
    "    #place wall\n",
    "    state[2,2] = np.array([0,0,1,0])\n",
    "    #place pit\n",
    "    state[1,1] = np.array([0,1,0,0])\n",
    "    #place goal\n",
    "    state[3,3] = np.array([1,0,0,0])\n",
    "\n",
    "    return state\n",
    "\n",
    "#Initialize player in random location, but keep wall, goal and pit stationary\n",
    "def initGridPlayer():\n",
    "    state = np.zeros((4,4,4))\n",
    "    #place player\n",
    "    state[randPair(0,4)] = np.array([0,0,0,1])\n",
    "    #place wall\n",
    "    state[2,2] = np.array([0,0,1,0])\n",
    "    #place pit\n",
    "    state[1,1] = np.array([0,1,0,0])\n",
    "    #place goal\n",
    "    state[1,2] = np.array([1,0,0,0])\n",
    "\n",
    "    a = findLoc(state, np.array([0,0,0,1])) #find grid position of player (agent)\n",
    "    w = findLoc(state, np.array([0,0,1,0])) #find wall\n",
    "    g = findLoc(state, np.array([1,0,0,0])) #find goal\n",
    "    p = findLoc(state, np.array([0,1,0,0])) #find pit\n",
    "    if (not a or not w or not g or not p):\n",
    "        #print('Invalid grid. Rebuilding..')\n",
    "        return initGridPlayer()\n",
    "\n",
    "    return state\n",
    "\n",
    "#Initialize grid so that goal, pit, wall, player are all randomly placed\n",
    "def initGridRand():\n",
    "    state = np.zeros((4,4,4))\n",
    "    #place player\n",
    "    state[randPair(0,4)] = np.array([0,0,0,1])\n",
    "    #place wall\n",
    "    state[randPair(0,4)] = np.array([0,0,1,0])\n",
    "    #place pit\n",
    "    state[randPair(0,4)] = np.array([0,1,0,0])\n",
    "    #place goal\n",
    "    state[randPair(0,4)] = np.array([1,0,0,0])\n",
    "\n",
    "    a = findLoc(state, np.array([0,0,0,1]))\n",
    "    w = findLoc(state, np.array([0,0,1,0]))\n",
    "    g = findLoc(state, np.array([1,0,0,0]))\n",
    "    p = findLoc(state, np.array([0,1,0,0]))\n",
    "    #If any of the \"objects\" are superimposed, just call the function again to re-place\n",
    "    if (not a or not w or not g or not p):\n",
    "        #print('Invalid grid. Rebuilding..')\n",
    "        return initGridRand()\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeMove(state, action):\n",
    "    #need to locate player in grid\n",
    "    #need to determine what object (if any) is in the new grid spot the player is moving to\n",
    "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
    "    wall = findLoc(state, np.array([0,0,1,0]))\n",
    "    goal = findLoc(state, np.array([1,0,0,0]))\n",
    "    pit = findLoc(state, np.array([0,1,0,0]))\n",
    "    state = np.zeros((4,4,4))\n",
    "\n",
    "    #up (row - 1)\n",
    "    if action==0:\n",
    "        new_loc = (player_loc[0] - 1, player_loc[1])\n",
    "        if (new_loc != wall):\n",
    "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
    "                state[new_loc][3] = 1\n",
    "    #down (row + 1)\n",
    "    elif action==1:\n",
    "        new_loc = (player_loc[0] + 1, player_loc[1])\n",
    "        if (new_loc != wall):\n",
    "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
    "                state[new_loc][3] = 1\n",
    "    #left (column - 1)\n",
    "    elif action==2:\n",
    "        new_loc = (player_loc[0], player_loc[1] - 1)\n",
    "        if (new_loc != wall):\n",
    "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
    "                state[new_loc][3] = 1\n",
    "    #right (column + 1)\n",
    "    elif action==3:\n",
    "        new_loc = (player_loc[0], player_loc[1] + 1)\n",
    "        if (new_loc != wall):\n",
    "            if ((np.array(new_loc) <= (3,3)).all() and (np.array(new_loc) >= (0,0)).all()):\n",
    "                state[new_loc][3] = 1\n",
    "\n",
    "    new_player_loc = findLoc(state, np.array([0,0,0,1]))\n",
    "\n",
    "    #if superimposed, return to original position\n",
    "    if (not new_player_loc):\n",
    "        state[player_loc] = np.array([0,0,0,1])\n",
    "        \n",
    "    #re-place pit\n",
    "    state[pit][1] = 1\n",
    "    #re-place wall\n",
    "    state[wall][2] = 1\n",
    "    #re-place goal\n",
    "    state[goal][0] = 1\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLoc(state, level):\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            if (state[i,j][level] == 1):\n",
    "                return i,j\n",
    "\n",
    "def getReward(state):\n",
    "    player_loc = getLoc(state, 3)\n",
    "    pit = getLoc(state, 1)\n",
    "    goal = getLoc(state, 0)\n",
    "    if (player_loc == pit):\n",
    "        return -10\n",
    "    elif (player_loc == goal):\n",
    "        return 10\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def dispGrid(state):\n",
    "    grid = np.zeros((4,4), dtype='<U2')\n",
    "    player_loc = findLoc(state, np.array([0,0,0,1]))\n",
    "    wall = findLoc(state, np.array([0,0,1,0]))\n",
    "    goal = findLoc(state, np.array([1,0,0,0]))\n",
    "    pit = findLoc(state, np.array([0,1,0,0]))\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            grid[i,j] = ' '\n",
    "\n",
    "    if player_loc:\n",
    "        grid[player_loc] = 'P' #player\n",
    "    if wall:\n",
    "        grid[wall] = 'W' #wall\n",
    "    if goal:\n",
    "        grid[goal] = '+' #goal\n",
    "    if pit:\n",
    "        grid[pit] = '-' #pit\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[u'-', u' ', u' ', u' '],\n",
       "       [u' ', u'W', u' ', u' '],\n",
       "       [u' ', u' ', u'+', u' '],\n",
       "       [u' ', u' ', u' ', u'P']], \n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = initGridRand()\n",
    "dispGrid(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-feae0de1fde3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdispGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4caec98e6c0b>\u001b[0m in \u001b[0;36mmakeMove\u001b[0;34m(state, action)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#left (column - 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mnew_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mplayer_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_loc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mwall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_loc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_loc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "state = makeMove(state, 2)\n",
    "dispGrid(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getReward(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(164, init='lecun_uniform', input_shape=(64,)))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2)) I'm not using dropout, but maybe you wanna give it a try?\n",
    "\n",
    "model.add(Dense(150, init='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(4, init='lecun_uniform'))\n",
    "model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='mse', optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16014339, -0.13676874, -0.11245299,  0.04404612]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(state.reshape(1,64), batch_size=1)\n",
    "#just to show an example output; read outputs left to right: up/down/left/right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup a for-loop to number of epochs\n",
    "2. In the loop, setup while loop (while game is in progress)\n",
    "3. Run Q network forward.\n",
    "4. We're using an epsilon greedy implementation, so at time t with probability  ϵϵ  we will choose a random action. With probability  1−ϵ1−ϵ  we will choose the action associated with the highest Q value from our neural network.\n",
    "5. Take action  aa  as determined in (4), observe new state  s′s′  and reward  rt+1rt+1 \n",
    "6. Run the network forward using  s′s′ . Store the highest Q value (maxQ).\n",
    "7. Our target value to train the network is reward + (gamma * maxQ) where gamma is a parameter ( 0<=γ<=10<=γ<=1 ).\n",
    "8. Given that we have 4 outputs and we only want to update/train the output associated with the action we just took, our target output vector is the same as the output vector from the first run, except we change the one output associated with our action to: reward + (gamma * maxQ)\n",
    "9. Train the model on this 1 sample. Repeat process 2-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "gamma = 0.9 #since it may take several moves to goal, making gamma high\n",
    "epsilon = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 999\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s - loss: 0.0236\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    state = initGrid()\n",
    "    status = 1\n",
    "    #while game still in progress\n",
    "    while(status == 1):\n",
    "        #We are in state S\n",
    "        #Let's run our Q function on S to get Q values for all possible actions\n",
    "        qval = model.predict(state.reshape(1,64), batch_size=1)\n",
    "        if (random.random() < epsilon): #choose random action\n",
    "            action = np.random.randint(0,4)\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state = makeMove(state, action)\n",
    "        #Observe reward\n",
    "        reward = getReward(new_state)\n",
    "        #Get max_Q(S',a)\n",
    "        newQ = model.predict(new_state.reshape(1,64), batch_size=1)\n",
    "        maxQ = np.max(newQ)\n",
    "        y = np.zeros((1,4))\n",
    "        y[:] = qval[:]\n",
    "        if reward == -1: #non-terminal state\n",
    "            update = (reward + (gamma * maxQ))\n",
    "        else: #terminal state\n",
    "            update = reward\n",
    "        y[0][action] = update #target output\n",
    "        print(\"Game #: %s\" % (i,))\n",
    "        model.fit(state.reshape(1,64), y, batch_size=1, nb_epoch=1, verbose=1)\n",
    "        state = new_state\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "        clear_output(wait=True)\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1./epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testAlgo(init=0):\n",
    "    i = 0\n",
    "    if init==0:\n",
    "        state = initGrid()\n",
    "    elif init==1:\n",
    "        state = initGridPlayer()\n",
    "    elif init==2:\n",
    "        state = initGridRand()\n",
    "\n",
    "    print(\"Initial State:\")\n",
    "    print(dispGrid(state))\n",
    "    status = 1\n",
    "    #while game still in progress\n",
    "    while(status == 1):\n",
    "        qval = model.predict(state.reshape(1,64), batch_size=1)\n",
    "        action = (np.argmax(qval)) #take action with highest Q-value\n",
    "        print('Move #: %s; Taking action: %s' % (i, action))\n",
    "        state = makeMove(state, action)\n",
    "        print(dispGrid(state))\n",
    "        reward = getReward(state)\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "            print(\"Reward: %s\" % (reward,))\n",
    "        i += 1 #If we're taking more than 10 actions, just stop, we probably can't win this game\n",
    "        if (i > 10):\n",
    "            print(\"Game lost; too many moves.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[u' ' u'P' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 0; Taking action: 3\n",
      "[[u' ' u' ' u'P' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 1; Taking action: 3\n",
      "[[u' ' u' ' u' ' u'P']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 2; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u'P']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 3; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u'P']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 4; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "testAlgo(init=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the harder variant, catastrophic forgetting, and experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=rms)#reset weights of neural network\n",
    "epochs = 3000\n",
    "gamma = 0.975\n",
    "epsilon = 1.\n",
    "batchSize = 40\n",
    "buffer = 80\n",
    "replay = [] #stores tuples of (S, A, R, S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 2999\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s - loss: 7.1033e-04\n"
     ]
    }
   ],
   "source": [
    "h = 0\n",
    "for i in range(epochs):\n",
    "\n",
    "    state = initGridPlayer() #using the harder state initialization function\n",
    "    status = 1\n",
    "    #while game still in progress\n",
    "    while(status == 1):\n",
    "        #We are in state S\n",
    "        #Let's run our Q function on S to get Q values for all possible actions\n",
    "        qval = model.predict(state.reshape(1,64), batch_size=1)\n",
    "        if (random.random() < epsilon): #choose random action\n",
    "            action = np.random.randint(0,4)\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state = makeMove(state, action)\n",
    "        #Observe reward\n",
    "        reward = getReward(new_state)\n",
    "\n",
    "        #Experience replay storage\n",
    "        if (len(replay) < buffer): #if buffer not filled, add to it\n",
    "            replay.append((state, action, reward, new_state))\n",
    "        else: #if buffer full, overwrite old values\n",
    "            if (h < (buffer-1)):\n",
    "                h += 1\n",
    "            else:\n",
    "                h = 0\n",
    "            replay[h] = (state, action, reward, new_state)\n",
    "            #randomly sample our experience replay memory\n",
    "            minibatch = random.sample(replay, batchSize)\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            for memory in minibatch:\n",
    "                #Get max_Q(S',a)\n",
    "                old_state, action, reward, new_state = memory\n",
    "                old_qval = model.predict(old_state.reshape(1,64), batch_size=1)\n",
    "                newQ = model.predict(new_state.reshape(1,64), batch_size=1)\n",
    "                maxQ = np.max(newQ)\n",
    "                y = np.zeros((1,4))\n",
    "                y[:] = old_qval[:]\n",
    "                if reward == -1: #non-terminal state\n",
    "                    update = (reward + (gamma * maxQ))\n",
    "                else: #terminal state\n",
    "                    update = reward\n",
    "                y[0][action] = update\n",
    "                X_train.append(old_state.reshape(64,))\n",
    "                y_train.append(y.reshape(4,))\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            print(\"Game #: %s\" % (i,))\n",
    "            model.fit(X_train, y_train, batch_size=batchSize, nb_epoch=1, verbose=1)\n",
    "            state = new_state\n",
    "        if reward != -1: #if reached terminal state, update game status\n",
    "            status = 0\n",
    "        clear_output(wait=True)\n",
    "    if epsilon > 0.1: #decrement epsilon over time\n",
    "        epsilon -= (1./epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[u' ' u' ' u' ' u'P']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 0; Taking action: 2\n",
      "[[u' ' u' ' u'P' u' ']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 1; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "testAlgo(1) #run testAlgo using random player placement => initGridPlayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
