{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1335)  # for reproducibility\n",
    "np.set_printoptions(precision=5, suppress=True, linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import backtest as twp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "def load_data():\n",
    "    price = np.arange(20) #linearly increasing prices\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize first state, all items are placed deterministically\n",
    "def init_state(data):\n",
    "    \n",
    "    close = data\n",
    "    diff = np.diff(data)\n",
    "    diff = np.insert(diff, 0, 0)\n",
    "    \n",
    "    #--- Preprocess data\n",
    "    xdata = np.column_stack((close, diff))\n",
    "    xdata = np.nan_to_num(xdata)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    xdata = scaler.fit_transform(xdata)\n",
    "    \n",
    "    state = xdata[0:1, :]\n",
    "    return state, xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take Action\n",
    "def take_action(state, xdata, action, signal, time_step):\n",
    "    #this should generate a list of trade signals that at evaluation time are fed to the backtester\n",
    "    #the backtester should get a list of trade signals and a list of price data for the asset\n",
    "    \n",
    "    #make necessary adjustments to state and then return it\n",
    "    time_step += 1\n",
    "    \n",
    "    #if the current iteration is the last state (\"terminal state\") then set terminal_state to 1\n",
    "    if time_step == xdata.shape[0]:\n",
    "        state = xdata[time_step-1:time_step, :]\n",
    "        terminal_state = 1\n",
    "        signal.loc[time_step] = 0\n",
    "        return state, time_step, signal, terminal_state\n",
    "\n",
    "    #move the market data window one step forward\n",
    "    state = xdata[time_step-1:time_step, :]\n",
    "    #take action 1 for long? 2 for short? 3 for stay? what about 0??\n",
    "    if action != 0:\n",
    "        if action == 1:\n",
    "            signal.loc[time_step] = 100\n",
    "        elif action == 2:\n",
    "            signal.loc[time_step] = -100\n",
    "        elif action == 3:\n",
    "            signal.loc[time_step] = 0\n",
    "    \n",
    "    terminal_state = 0\n",
    "\n",
    "    return state, time_step, signal, terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Reward, the reward is returned at the end of an episode\n",
    "def get_reward(new_state, time_step, action, xdata, signal, terminal_state, epoch=0): # \"epoch=0\" migght be useless\n",
    "    reward = 0\n",
    "    signal.fillna(value=0, inplace=True)\n",
    "    if terminal_state == 0:\n",
    "        #get reward for the most current action\n",
    "        if signal[time_step] != signal[time_step-1] and terminal_state == 0:\n",
    "            i=1\n",
    "            while signal[time_step-i] == signal[time_step-1-i] and time_step - 1 - i > 0:\n",
    "                i += 1\n",
    "            reward = (xdata[time_step-1, 0] - xdata[time_step - i-1, 0]) * signal[time_step - 1] * -100. + i*np.abs(signal[time_step - 1])/10.\n",
    "        if signal[time_step] == 0 and signal[time_step - 1] == 0:\n",
    "            reward -= 10\n",
    "\n",
    "    #calculate the reward for all actions if the last iteration in set\n",
    "    if terminal_state == 1:\n",
    "        #run backtest, send list of trade signals and asset data to backtest function\n",
    "        bt = twp.Backtest(pd.Series(data=[x[0] for x in xdata]), signal, signalType='shares')\n",
    "        reward = bt.pnl.iloc[-1]\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indata = load_data()\n",
    "state, xdata = init_state(indata)\n",
    "signal = pd.Series(index=np.arange(len(indata)))\n",
    "time_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.64751, -4.3589 ],\n",
       "       [-1.47409,  0.22942],\n",
       "       [-1.30066,  0.22942],\n",
       "       [-1.12724,  0.22942],\n",
       "       [-0.95382,  0.22942],\n",
       "       [-0.7804 ,  0.22942],\n",
       "       [-0.60698,  0.22942],\n",
       "       [-0.43355,  0.22942],\n",
       "       [-0.26013,  0.22942],\n",
       "       [-0.08671,  0.22942],\n",
       "       [ 0.08671,  0.22942],\n",
       "       [ 0.26013,  0.22942],\n",
       "       [ 0.43355,  0.22942],\n",
       "       [ 0.60698,  0.22942],\n",
       "       [ 0.7804 ,  0.22942],\n",
       "       [ 0.95382,  0.22942],\n",
       "       [ 1.12724,  0.22942],\n",
       "       [ 1.30066,  0.22942],\n",
       "       [ 1.47409,  0.22942],\n",
       "       [ 1.64751,  0.22942]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(xdata).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.64751,  0.22942]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.64751  0.22942]] 20 0       0.0\n",
      "1       0.0\n",
      "2     100.0\n",
      "3     100.0\n",
      "4     100.0\n",
      "5    -100.0\n",
      "6    -100.0\n",
      "7    -100.0\n",
      "8    -100.0\n",
      "9       0.0\n",
      "10      0.0\n",
      "11      0.0\n",
      "12      0.0\n",
      "13      0.0\n",
      "14   -100.0\n",
      "15   -100.0\n",
      "16   -100.0\n",
      "17    100.0\n",
      "18    100.0\n",
      "19    100.0\n",
      "20      0.0\n",
      "dtype: float64 1\n"
     ]
    }
   ],
   "source": [
    "new_state, time_step, signal, terminal_state = take_action(state, xdata, 1, signal, time_step)\n",
    "print new_state, time_step, signal, terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.684398781\n"
     ]
    }
   ],
   "source": [
    "reward = get_reward(new_state, time_step, 1, xdata, signal, terminal_state)\n",
    "print reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.64751, -4.3589 ],\n",
       "       [-1.47409,  0.22942],\n",
       "       [-1.30066,  0.22942],\n",
       "       [-1.12724,  0.22942],\n",
       "       [-0.95382,  0.22942],\n",
       "       [-0.7804 ,  0.22942],\n",
       "       [-0.60698,  0.22942],\n",
       "       [-0.43355,  0.22942],\n",
       "       [-0.26013,  0.22942],\n",
       "       [-0.08671,  0.22942],\n",
       "       [ 0.08671,  0.22942],\n",
       "       [ 0.26013,  0.22942],\n",
       "       [ 0.43355,  0.22942],\n",
       "       [ 0.60698,  0.22942],\n",
       "       [ 0.7804 ,  0.22942],\n",
       "       [ 0.95382,  0.22942],\n",
       "       [ 1.12724,  0.22942],\n",
       "       [ 1.30066,  0.22942],\n",
       "       [ 1.47409,  0.22942],\n",
       "       [ 1.64751,  0.22942]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if terminal_state == 0:\n",
    "    #get reward for the most current action\n",
    "    if signal[time_step] != signal[time_step-1] and terminal_state == 0:\n",
    "        i=1\n",
    "        while signal[time_step-i] == signal[time_step-1-i] and time_step - 1 - i > 0:\n",
    "            i += 1\n",
    "        reward = (xdata[time_step-1, 0] - xdata[time_step -i-1, 0]) * signal[time_step - 1] * -100. + i*np.abs(signal[time_step - 1])/10.\n",
    "    if signal[time_step] == 0 and signal[time_step - 1] == 0:\n",
    "        reward -= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal[time_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17342199390482405"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xdata[time_step-1, 0] - xdata[time_step - i-1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal[time_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4740869481910039"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata[time_step-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.64751, -4.3589 ],\n",
       "       [-1.47409,  0.22942],\n",
       "       [-1.30066,  0.22942],\n",
       "       [-1.12724,  0.22942],\n",
       "       [-0.95382,  0.22942],\n",
       "       [-0.7804 ,  0.22942],\n",
       "       [-0.60698,  0.22942],\n",
       "       [-0.43355,  0.22942],\n",
       "       [-0.26013,  0.22942],\n",
       "       [-0.08671,  0.22942],\n",
       "       [ 0.08671,  0.22942],\n",
       "       [ 0.26013,  0.22942],\n",
       "       [ 0.43355,  0.22942],\n",
       "       [ 0.60698,  0.22942],\n",
       "       [ 0.7804 ,  0.22942],\n",
       "       [ 0.95382,  0.22942],\n",
       "       [ 1.12724,  0.22942],\n",
       "       [ 1.30066,  0.22942],\n",
       "       [ 1.47409,  0.22942],\n",
       "       [ 1.64751,  0.22942]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below is just helper function\n",
    "def evaluate_Q(eval_data, eval_model):\n",
    "    #This function is used to evaluate the perofrmance of the system each epoch, without the influence of epsilon and random actions\n",
    "    signal = pd.Series(index=np.arange(len(eval_data)))\n",
    "    state, xdata = init_state(eval_data)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    time_step = 1\n",
    "    while(status == 1):\n",
    "        #We start in state S\n",
    "        #Run the Q function on S to get predicted reward values on all the possible actions\n",
    "        qval = eval_model.predict(state.reshape(1,2), batch_size=1)\n",
    "        action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        #Observe reward\n",
    "        eval_reward = get_reward(new_state, time_step, action, xdata, signal, terminal_state, i)\n",
    "        state = new_state\n",
    "        if terminal_state == 1: #terminal state\n",
    "            status = 0\n",
    "    return eval_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(4, init='lecun_uniform', input_shape=(2,)))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2)) I'm not using dropout in this example\n",
    "\n",
    "model.add(Dense(4, init='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(4, init='lecun_uniform'))\n",
    "model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='mse', optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "indata = load_data()\n",
    "epochs = 10\n",
    "gamma = 0.9 #a high gamma makes a long term reward more valuable\n",
    "epsilon = 1.\n",
    "learning_progress = []\n",
    "#stores tuples of (S, A, R, S')\n",
    "h = 0\n",
    "signal = pd.Series(index=np.arange(len(indata)))\n",
    "for i in range(epochs):\n",
    "\n",
    "    state, xdata = init_state(indata)\n",
    "    status = 1\n",
    "    terminal_state = 0\n",
    "    time_step = 1\n",
    "    #while learning is still in progress\n",
    "    while(status == 1):\n",
    "        #We start in state S\n",
    "        #Run the Q function on S to get predicted reward values on all the possible actions\n",
    "        qval = model.predict(state.reshape(1,2), batch_size=1)\n",
    "        if (random.random() < epsilon) and i != epochs - 1: #maybe choose random action if not the last epoch\n",
    "            action = np.random.randint(0,4) #assumes 4 different actions\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            action = (np.argmax(qval))\n",
    "        #Take action, observe new state S'\n",
    "        new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        #Observe reward\n",
    "        reward = get_reward(new_state, time_step, action, xdata, signal, terminal_state, i)\n",
    "        #Get max_Q(S',a)\n",
    "        newQ = model.predict(new_state.reshape(1,2), batch_size=1)\n",
    "        maxQ = np.max(newQ)\n",
    "        y = np.zeros((1,4))\n",
    "        y[:] = qval[:]\n",
    "        if terminal_state == 0: #non-terminal state\n",
    "            update = (reward + (gamma * maxQ))\n",
    "        else: #terminal state (means that it is the last state)\n",
    "            update = reward\n",
    "        y[0][action] = update #target output\n",
    "        model.fit(state.reshape(1,2), y, batch_size=1, nb_epoch=1, verbose=0)\n",
    "        state = new_state\n",
    "        if terminal_state == 1: #terminal state\n",
    "            status = 0\n",
    "    eval_reward = evaluate_Q(indata, model)\n",
    "    print(\"Epoch #: %s Reward: %f Epsilon: %f\" % (i,eval_reward, epsilon))\n",
    "    learning_progress.append((eval_reward))\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1.0/epochs)\n",
    "\n",
    "elapsed = np.round(timeit.default_timer() - start_time, decimals=2)\n",
    "print(\"Completed in %f\" % (elapsed,))\n",
    "\n",
    "#plot results\n",
    "bt = twp.Backtest(pd.Series(data=[x[0] for x in xdata]), signal, signalType='shares')\n",
    "bt.data['delta'] = bt.data['shares'].diff().fillna(0)\n",
    "\n",
    "print(bt.data)\n",
    "\n",
    "plt.figure()\n",
    "bt.plotTrades()\n",
    "plt.suptitle('epoch' + str(i))\n",
    "plt.savefig('plt/final_trades'+'.png', bbox_inches='tight', pad_inches=1, dpi=72) #assumes there is a ./plt dir\n",
    "plt.close('all')\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "bt.plotTrades()\n",
    "plt.subplot(3,1,2)\n",
    "bt.pnl.plot(style='x-')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(learning_progress)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
